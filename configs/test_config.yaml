
#If True it will launch wandb
deploy: True
# Collect experiments together. Name of log file will be wandb run id
tag: scratch
seed: 0
# default config values designed to train a gpt2 (124M) on OpenWebText
# I/O
out_dir : 'output'
eval_interval : 100
log_interval : 100
save_ckpt_interval : 100
eval_iters : 200
eval_only : False # if True, script exits right after the first eval
always_save_checkpoint : True # if True, always save a checkpoint after each eval
init_from : 'scratch' # 'scratch' or 'resume' or 'gpt2*'
# wandb logging
# wandb_log = False # disabled by default
wandb_project : 'MoE'
wandb_run_name : 'test' 
# data
dataset_path : 'data/'
vocab_size : 100
gradient_accumulation_steps : 1 #40 # used to simulate larger batch sizes
batch_size : 64 # if gradient_accumulation_steps > 1, this is the micro-batch size
# model parameters
block_size : 128
n_layer : 1
n_head : 1
n_embd : 64
num_experts: 8
expert_k: 1
noisy_gating: true
dropout : 0.0 # for pretraining 0 is good, for finetuning try 0.1+
bias : False # do we use bias inside LayerNorm and Linear layers?
# adamw optimizer
learning_rate : 6e-4 # max learning rate
max_iters : 40000 # total number of training iterations
optimizer : 'AdamW'
weight_decay : 0.1
beta1 : 0.9
beta2 : 0.95
grad_clip : 1.0 # clip gradients at this value, or disable if == 0.0
# learning rate decay settings
decay_lr : True # whether to decay the learning rate
warmup_iters : 20 # how many steps to warm up for
lr_decay_iters : 600000 # should be ~= max_iters per Chinchilla
min_lr : 6e-4 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla
# DDP settings
backend : 'nccl' # 'nccl', 'gloo', etc.
# system
device : 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
compile : False # use PyTorch 2.0 to compile the model to be faster
top_k : 10
temperature : 1 
num_samples_generated_for_accuracy : 1

# Nested configs. Disable hydra logging
defaults:
  - _self_
  # - hp: cifar10  # Use this for nested file directories
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

# Disable hydra directory structure
hydra:
  output_subdir: Null
  run:
    dir: .

  sweep:
    dir: .
    subdir: .
